{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 CourierNewPSMT;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue255;}
{\*\expandedcolortbl;;\csgenericrgb\c0\c0\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid201\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid301\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid401\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid501\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid601\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid701\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid801\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0

\f0\fs22 \cf0 #Task 3 \'96 Data Cleaning  \
#The process of data cleaning is carried out in Pig. First of all the dataset which is to be cleaned should be copied from the bucket to the local terminal disk space/Hadoop system. This will be done by the following command\
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls1\ilvl0\cf0 1.	hadoop fs -cp 'gs://dataproc-staging-europe-west1-773978562921-grrhd7uh/assignment1DataBucket/ElectronicsCSV1.csv' /DataCSV \
\pard\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 #One the data is copied, start the pig. Once the Pig is starts, include the library of piggybank.jar. I included this file through my GitHub with wget command in hadoop directory as follows:\
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls2\ilvl0\cf0 2.	wget \cf2 \ul \ulc2 https://github.com/hamzaqadeer1/CA-675---Cloud-Technologies-Assignment-1/blob/main/piggybank.jar\cf0 \ulnone \
\pard\pardeftab720\li720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 \
\pard\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 #then I was able to register this in my Pig cleaning session. \
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls3\ilvl0\cf0 3.	register /home/sachin_patel3/pig/piggybank.jar \
\pard\pardeftab720\li720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 \
\pard\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 # Then starts the reading and loading of the data from Hadoop file system in Pig by including hdfs path and columns names along with their datatypes. Command for Loading data with the following command. The command used is as follows:\
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls4\ilvl0\cf0 4.	newElectronicsCSV1_pigLoad = Load 'hdfs://cluster-assignment1-m/user/hamza_qadeer2/LabTasks/DataCSV/ElectronicsCSV1.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',','YES_MULTILINE') AS (sno:chararray,id:chararray,reviewerID:chararray,asin:chararray,reviewerName:chararray,helpful:chararray,reviewText:chararray,overall:chararray,summary:chararray,unixReviewTime:chararray,reviewTime:chararray,category:chararray,class:chararray);\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 #After this command the data loaded was checked then check for NULL values present in the data through the following query. Though I checked the in initial exploration as well but it\'92s tried here again as a requirement of this task.\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls5\ilvl0\cf0 5.	checkNULL = FILTER q1newFileData by NOT ((sno IS NULL) OR (reviewerID IS NULL) OR (asin IS NULL) OR (reviewerName IS NULL) OR (helpful IS NULL) OR (reviewText IS NULL) OR (overall IS NULL) OR (summary IS NULL));\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 #After checking the NULL values, I will now check the blank values in the dataset by the following query \
\pard\pardeftab720\li720\fi-360\ri-340\sb100\sa100\partightenfactor0
\ls6\ilvl0\cf0 6.	checkBlank1 = FILTER checkNULL by NOT  ((reviewerID =='') OR (asin =='') OR (reviewerName =='') OR (helpful =='') OR (reviewText =='') OR (overall =='') OR (summary =='')); \
\pard\pardeftab720\li360\ri-340\sb100\sa100\partightenfactor0
\cf0 Once the blank values are confirmed, then we will check the \'91N/A\'92 values if present\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\fi-360\ri-340\partightenfactor0
\ls7\ilvl0\cf0 7.	checkNA = FILTER checkBlank1 by NOT ((sno =='N/A') OR (reviewerID =='NA') OR (asin =='N/A') OR (reviewerName =='N/A') OR (helpful =='N/A') OR (reviewText =='N/A') OR (overall =='N/A') OR (summary =='N/A'));\
\pard\pardeftab720\ri-340\sl259\slmult1\sa160\partightenfactor0
\cf0 \
#At this step we will store the values in newly created file. \
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sa160\partightenfactor0
\ls8\ilvl0\cf0 8.	STORE checkNA INTO '/Electronics_Clean_Data' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',','YES_MULTILINE');\
\pard\pardeftab720\ri-340\sl288\slmult1\sa160\partightenfactor0
\cf0 \
\pard\pardeftab720\ri-340\sl259\slmult1\sb100\sa100\partightenfactor0
\cf0 #After success message of output creation, we would need to quit pig and get back on Hadoop. Where we will first locate those files which were created and then, merge those chunks of output files created by pig cleaning process. \
\pard\pardeftab720\li720\fi-360\ri-340\sl259\slmult1\sb100\sa100\partightenfactor0
\ls9\ilvl0\cf0 9.	hadoop fs -getmerge /Electronics_Clean_Data/ /home/hamza_qadeer2/CleanData/MergeCleanElectronics.csv \
10.	hadoop fs -put MergeCleanElectronics.csv 'gs://dataproc-staging-europe-west1-773978562921-grrhd7uh/assignment1DataBucket/'\
}